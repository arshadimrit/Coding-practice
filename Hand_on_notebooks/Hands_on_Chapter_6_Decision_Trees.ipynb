{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as graph\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=2, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, 2:] # petal length and width\n",
    "y = iris.target\n",
    "\n",
    "tree_clf = DecisionTreeClassifier(max_depth=2)\n",
    "tree_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "export_graphviz(\n",
    "        tree_clf,\n",
    "        out_file=\"/mnt/c/Users/imrit/Downloads/iris_tree.dot\",\n",
    "        feature_names=iris.feature_names[2:],\n",
    "        class_names=iris.target_names,\n",
    "        rounded=True,\n",
    "        filled=True\n",
    ")\n",
    "\n",
    "# dot -Tpng iris_tree.dot -o iris_tree.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Class Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.90740741 0.09259259]]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(tree_clf.predict_proba([[5, 1.5]]))\n",
    "print(tree_clf.predict([[5, 1.5]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART Training Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Classification and regression tree , growing tree\n",
    "- split in 2 subsets, using single feature k and threshold tk\n",
    "    - choose K and tk using combinations of k, tk that produces purest subset (weighted by size)\n",
    "- Cost function: some ratio of gini/impurity of left vs right subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gini Impurity or Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Entropy = Zero when all info is the same, so same instances\n",
    "- so gini and entropy pretty similar\n",
    "    - although gini faster but entropy produces more balances tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularizing hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- nonparametric because number of parameters not determined prior to training\n",
    "    - hence tend to overfit\n",
    "- regularize by:\n",
    "    - set number of samples per leaf\n",
    "    - max number of leaves/nodes\n",
    "    - ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split so that most training instances as close as possible to the predicted value\n",
    "- Not minimize gini here, but minimize MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=2,\n",
       "                      max_features=None, max_leaf_nodes=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                      random_state=None, splitter='best')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - love orthogonal tree\n",
    " - PCA (will return better orientation) and then decision tree can make better\n",
    " - sensitive to small variations in data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Train and fine-tune a Decision Tree for the moons dataset.\n",
    "    - a. Generate a moons dataset using make_moons(n_samples=10000, noise=0.4).\n",
    "    - b. Split it into a training set and a test set using train_test_split().\n",
    "    - c. Use grid search with cross-validation (with the help of the GridSearchCV\n",
    "        class) to find good hyperparameter values for a DecisionTreeClassifier.\n",
    "        Hint: try various values for max_leaf_nodes.\n",
    "    - d. Train it on the full training set using these hyperparameters, and measure\n",
    "        your model’s performance on the test set. You should get roughly 85% to 87%\n",
    "        accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Grow a forest.\n",
    "   - a. Continuing the previous exercise, generate 1,000 subsets of the training set,\n",
    "    each containing 100 instances selected randomly. Hint: you can use Scikit-\n",
    "    Learn’s ShuffleSplit class for this.\n",
    "    - b. Train one Decision Tree on each subset, using the best hyperparameter values\n",
    "    found above. Evaluate these 1,000 Decision Trees on the test set. Since they\n",
    "    were trained on smaller sets, these Decision Trees will likely perform worse\n",
    "    than the first Decision Tree, achieving only about 80% accuracy.\n",
    "    - c. Now comes the magic. For each test set instance, generate the predictions of\n",
    "    the 1,000 Decision Trees, and keep only the most frequent prediction (you can\n",
    "    use SciPy’s mode() function for this). This gives you majority-vote predictions\n",
    "    over the test set.\n",
    "    - d. Evaluate these predictions on the test set: you should obtain a slightly higher\n",
    "    accuracy than your first model (about 0.5 to 1.5% higher). Congratulations,\n",
    "    you have trained a Random Forest classifier!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
