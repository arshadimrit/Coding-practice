{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as graph\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime\n",
    "now = datetime.date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link MySQL and Python\n",
    "We want to connect MySQL and Python to read the database in Python directly. Using mysql connector seems appropriate here. Let's write a class that will load the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MysqlIO:\n",
    "    def __init__(self, database='test'):\n",
    "            connection = mysql.connector.connect(host='127.0.0.1', \n",
    "                                                 port='3306',\n",
    "                                                 database=database,\n",
    "                                                 user='root',\n",
    "                                                 password='ForSQL!1',\n",
    "                                                 use_pure=True\n",
    "                                                 )\n",
    "            db_info = connection.get_server_info()\n",
    "            self.connection = connection\n",
    "            print(f\"Retrieval of database {database} Successful\")\n",
    "            \n",
    "    def execute(self, query, header=False):\n",
    "        \"\"\"Execute SQL commands and return retrieved queries.\"\"\"\n",
    "        cursor = self.connection.cursor(buffered=True)\n",
    "        cursor.execute(query)\n",
    "        try:\n",
    "            record = cursor.fetchall()\n",
    "            if header:\n",
    "                header = [i[0] for i in cursor.description]\n",
    "                return {'header': header, 'record': record}\n",
    "            else:    \n",
    "                return record\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    def to_df(self, query):\n",
    "        \"\"\"Return the retrieved SQL queries into pandas dataframe\"\"\"\n",
    "        res = self.execute(query, header=True)\n",
    "        df = pd.DataFrame(res['record'])\n",
    "        df.columns = res['header']\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval of database bank_1999 Successful\n"
     ]
    }
   ],
   "source": [
    "db = MysqlIO('bank_1999')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See WeCloudData_Loan_Default_Data_Exploration.ipynb for more details\n",
    "- Target variable is 'status' from loan table\n",
    "- Predictors are:\n",
    "    - 'amount', 'duration', 'payments', 'frequency', 'loan_days' from loan and account table\n",
    "    - 'region', 'num_inhabitants', 'ratio_urban_inhabitants', 'avg_salary', 'unemployment_rate', 'entrepreneur_rate', 'crime_rate' from district table\n",
    "    - 'avg_order_amount from order table\n",
    "    - 'avg_bal_amount', 'avg_trans_amount', and 'num_trans' from transactions table\n",
    "    - 'age', 'gender' from client table\n",
    "    - 'card_type' from card table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loan, Account, District Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    select loan_t.date as loan_date, account_t.date as account_date,\n",
    "    account_id, amount, duration, payments, status, frequency, \n",
    "    A3, A4, A10, A11, A12, A13, A14, A15, A16\n",
    "    from loan_t join account_t using(account_id)\n",
    "    join demographic_t using(district_id);\n",
    "    \"\"\"\n",
    "loan_account_district_df = db.to_df(query)\n",
    "\n",
    "loan_account_district_df['defaulted'] = np.where(loan_account_district_df['status'].isin(['A', 'C']), 'no', 'yes')\n",
    "loan_account_district_df.replace({'POPLATEK MESICNE': 'monthly', 'POPLATEK TYDNE': 'weekly', 'POPLATEK PO OBRATU': 'instant'}, \n",
    "                        inplace=True)\n",
    "\n",
    "for col in ['loan_date', 'account_date']:\n",
    "    loan_account_district_df[col] = pd.to_datetime(loan_account_district_df[col]) \n",
    "\n",
    "loan_account_district_df['loan_days'] = (loan_account_district_df['loan_date'] - \n",
    "                                            loan_account_district_df['account_date']).dt.days\n",
    "\n",
    "# District Table Cleanup\n",
    "loan_account_district_df['unemployment_rate'] = loan_account_district_df[['A12', 'A13']].mean(axis=1)\n",
    "loan_account_district_df['crime_rate'] = loan_account_district_df[['A15', 'A16']].mean(axis=1) / loan_account_district_df['A4']\n",
    "\n",
    "loan_account_district_df = loan_account_district_df[['account_id', \n",
    "                                                     'status', 'amount', 'duration', 'payments', 'frequency', 'loan_days', \n",
    "                                                     'A3', 'A4', 'A10', 'A11', 'unemployment_rate', \n",
    "                                                     'A14', 'crime_rate']]\n",
    "loan_account_district_df.rename(columns = {'A3': 'region', 'A4': 'num_inhabitants', 'A10': 'ratio_urban_inhabitants', \n",
    "                              'A11': 'avg_salary', 'A14': 'entrepreneur_rate'}, inplace=True)\n",
    "\n",
    "loan_account_district_df['defaulted'] = np.where(loan_account_district_df['status'].isin(['A', 'C']), 'no', 'yes')\n",
    "display(loan_account_district_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Order table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    select amount as order_amount, account_id\n",
    "    from perm_order_t\n",
    "    where account_id in (select account_id from loan_t);\n",
    "    \"\"\"\n",
    "account_order_df = db.to_df(query)\n",
    "account_order_df['order_amount'] = account_order_df['order_amount'].astype(float)\n",
    "\n",
    "account_order_df = account_order_df.groupby('account_id').mean().reset_index()\n",
    "account_order_df.rename(columns = {'order_amount': 'avg_order_amount'}, inplace=True)\n",
    "\n",
    "display(account_order_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(loan_account_district_df, account_order_df, on='account_id')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transaction table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    select amount as trans_amount, balance as bal_amount, account_id\n",
    "    from transaction_t\n",
    "    where account_id in (select account_id from loan_t);\n",
    "    \"\"\"\n",
    "transaction_df = db.to_df(query)\n",
    "\n",
    "display(transaction_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transaction_df['number_trans'] = transaction_df.groupby('account_id')['account_id'].transform('count')\n",
    "transaction_df = transaction_df.groupby('account_id').mean().reset_index()\n",
    "transaction_df.rename(columns = {'trans_amount': 'avg_trans_amount'})\n",
    "display(transaction_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, transaction_df, on='account_id')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Card, disposition, client tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    select card_t.type as card_type, account_id, gender, birth_date, district_id, disposition_t.type as disp_type\n",
    "    from loan_t join disposition_t using(account_id)\n",
    "    left join card_t using(disp_id)\n",
    "    left join client_t using(client_id);\n",
    "    \"\"\"\n",
    "card_client_df = db.to_df(query)\n",
    "card_client_df = card_client_df.loc[card_client_df['disp_type'] == 'OWNER']\n",
    "card_client_df['card_type'].fillna('unknown', inplace=True)\n",
    "card_client_df['birth_date'] = pd.to_datetime(card_client_df['birth_date'])\n",
    "\n",
    "card_client_df['age'] = (pd.to_datetime(now) - card_client_df['birth_date']).dt.days // 365\n",
    "\n",
    "card_client_df = card_client_df[['card_type', 'account_id', 'gender', 'age']]\n",
    "display(card_client_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, card_client_df, on='account_id')\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['account_id', 'status'], inplace=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations to choose best predictors\n",
    "Although we did a lot of EDA, we did not have everything in one table. We can do some correlations to see which variables are highly associated with each other, which we could potentially drop and refine our list of predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.figure(figsize=(14, 14))\n",
    "sns.heatmap(corr_df, cmap='seismic', center=0, annot=True)\n",
    "graph.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although some are highly correlated, they do not really make intuitive sense. If we knew the data more, we could have explored further and choose our variables better. But we'll keep everything for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "Now we have all the data, we should scale and transform them, so that they are ready when we make our models.\n",
    "\n",
    "We have categorical and numerical variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, plot_roc_curve\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['defaulted']\n",
    "x = df[[col for col in df.columns if col != 'defaulted']]\n",
    "x_labels = x.columns\n",
    "\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "cat_x = x[['frequency', 'region', 'card_type', 'gender']]\n",
    "num_x = x[[col for col in x if col not in cat_x.columns]].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform target\n",
    "\n",
    "y = LabelEncoder().fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform X \n",
    "x_trans = ColumnTransformer([\n",
    "    ('cat', OneHotEncoder(), cat_x.columns), \n",
    "    ('num', StandardScaler(), num_x.columns)\n",
    "])\n",
    "\n",
    "x = x_trans.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "We can test several models here. Since we have binary data (for target), good options are logistic regressions, decision trees, random forest, Gradient Boosting.\n",
    "\n",
    "Logistic regressions will provide more statistical robustness. Random Forests are easy to understand and perform better than decision trees (less overfitting for example). Given we do not have a lot of data, we might not need to look into Gradient Boosting for now (but we will if we gave enough time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, features=x, true_y=y):\n",
    "    prediction = model.predict(features)\n",
    "    print(f'Accuracy score: {accuracy_score(true_y, prediction)}')\n",
    "    print(f'F1 score:       {f1_score(true_y, prediction)}')\n",
    "    print(f'ROC AUC score:  {roc_auc_score(true_y, prediction)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model = LogisticRegression()\n",
    "logit_model.fit(x_train, y_train)\n",
    "\n",
    "evaluate(logit_model)\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(logit_model, x_train, y_train)\n",
    "graph.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(x_train, y_train)\n",
    "\n",
    "evaluate(rf_model)\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(rf_model, x_train, y_train)\n",
    "graph.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_model = GradientBoostingClassifier()\n",
    "gb_model.fit(x_train, y_train)\n",
    "\n",
    "evaluate(gb_model)\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(gb_model, x_train, y_train)\n",
    "graph.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the random forest is better out of the 3 models we tested. We'll go with the random forest and fine tune it now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = GridSearchCV(\n",
    "    RandomForestClassifier(),\n",
    "    param_grid={\n",
    "        'n_estimators': [20, 100, 200], \n",
    "        'max_depth': [None, 10, 25],\n",
    "        'min_samples_split': [2, 5, 8],\n",
    "        'max_features': ['auto', 5, 8, 10]\n",
    "    },\n",
    "    n_jobs=-1,\n",
    "    cv=10,\n",
    ")\n",
    "\n",
    "rf_model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_model.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_rf_model = rf_model.best_estimator_\n",
    "evaluate(best_rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model improved slightly. With more time, we could have tested for more parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay.from_estimator(best_rf_model, x_train, y_train)\n",
    "graph.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see how the model performs on the test set now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(best_rf_model, x_test, y_test)\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(best_rf_model, x_test, y_test)\n",
    "graph.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, not great :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.figure(figsize=(12, 5))\n",
    "plot_roc_curve(best_rf_model, x_test, y_test)\n",
    "graph.plot([0, 1], [0, 1])\n",
    "graph.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels = list(x_trans.transformers_[0][1].get_feature_names()) + list(x_trans.transformers_[1][2])\n",
    "feature_importance = best_rf_model.feature_importances_\n",
    "\n",
    "features_df = pd.DataFrame({'features': feature_labels, 'importance': feature_importance}).sort_values(by='importance', \n",
    "                                                                                                       ascending=False)\n",
    "display(features_df[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the most important variables are the balance the client has, the average transaction amount of the client, payments. Interesting that demographic variables are not that important, but rather the attributes of the client's finances themselves"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
